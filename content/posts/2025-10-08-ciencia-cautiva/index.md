+++
date = '2025-10-07T22:06:55-03:00'
draft = false
title = 'Ciencia Cautiva: El Caso Timnit Gebru y la Censura en Big Tech'
slug = 'ciencia-cautiva'
tags = ['IA', '√©tica', 'big tech', 'investigaci√≥n']
+++

> "Have you ever heard of someone getting 'feedback' on a paper through a privileged and confidential document to HR? Or does it just happen to people like me who are constantly dehumanized?"
>
> ‚Äî Timnit Gebru, en email interno a Google Brain, noviembre 2020

El 2 de diciembre de 2020, mientras la industria tecnol√≥gica celebraba los avances de GPT-3 y el boom de los modelos de lenguaje masivos, un tweet deton√≥ una crisis que revel√≥ hasta qu√© punto Big Tech han llegado a controlar la investigaci√≥n en IA. Timnit Gebru, investigadora et√≠ope de Stanford y co-l√≠der del equipo de IA √©tica de Google, anunciaba su despido. El motivo: atreverse a cuestionar la carrera desenfrenada por modelos cada vez m√°s grandes.

La historia de Gebru no es un caso aislado de conflicto laboral. Es el s√≠ntoma de una transformaci√≥n profunda en c√≥mo se hace investigaci√≥n en IA: de la academia abierta a los laboratorios corporativos cerrados, de la cr√≠tica cient√≠fica a la complacencia comercial.

{{<figure src="TimnitGebru.jpg" caption="Timnit Gebru en 2018 (imagen tomada de Wikipedia)" >}}

Esta historia, que se detalla en el cap√≠tulo 7 del libro ["Empire of AI"](https://amzn.to/3KCK2Mh) de Karen Hao, es muy reveladora.

## El Paper que Molest√≥ a Google

Todo comenz√≥ con un paper llamado "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú" (*¬øPueden los modelos de lenguaje ser demasiado grandes?*). Gebru y sus coautoras ‚Äîentre ellas Emily Bender, profesora de ling√º√≠stica computacional en la Universidad de Washington‚Äî hab√≠an documentado cuatro problemas fundamentales de los modelos masivos de lenguaje:

1. **Impacto ambiental**: El entrenamiento de estos modelos genera grandes emisiones de carbono. En un planeta en crisis clim√°tica, esto no es un detalle menor.

2. **Basura t√≥xica a escala**: Al entrenar con todo lo disponible en internet, incluyendo Reddit, 4chan y foros que Gebru, como mujer negra, evitaba precisamente por el acoso sistem√°tico, estos modelos amplificaban sesgos racistas, sexistas y discursos de odio.

3. **Opacidad total**: Los datasets son tan masivos que resulta imposible auditarlos. Nadie sabe realmente qu√© contienen.

4. **La ilusi√≥n del significado**: Los outputs son tan buenos que la gente confunde patrones estad√≠sticos con comprensi√≥n real, convirtiendo a estos sistemas en supuestos "consejeros confiables".

Nada de esto era particularmente controversial en t√©rminos acad√©micos. De hecho, citaba investigaci√≥n previa bien establecida. Pero hab√≠a un problema: Google hab√≠a inventado el Transformer, la tecnolog√≠a base de GPT-3, y lo usaba en su producto m√°s lucrativo: Google Search.

Criticar los modelos masivos de lenguaje era criticar el futuro del negocio.

## El Proceso Kafkiano

Lo que ocurri√≥ despu√©s es una nos muestra c√≥mo opera la censura corporativa moderna. Una semana antes del D√≠a de Acci√≥n de Gracias de 2020, Gebru recibi√≥ una invitaci√≥n de calendario sin explicaci√≥n: reuni√≥n con Megan Kacholia, VP de ingenier√≠a de Google Research, en tres horas.

La reuni√≥n dur√≥ 30 minutos. Kacholia fue directa: Gebru deb√≠a retractar el paper.

No era una solicitud de revisi√≥n. No era un pedido de aclaraciones. Era una orden de retracci√≥n total.

Gebru pregunt√≥ lo obvio: ¬øQui√©n hab√≠a objetado? ¬øPod√≠a hablar con ellos? ¬øPod√≠a revisar secciones espec√≠ficas? ¬øPod√≠a publicarlo bajo otra afiliaci√≥n?

No, no y no.

Ten√≠a hasta el d√≠a despu√©s del feriado para retractarse. Gebru llor√≥ al final de la llamada.

En lugar de disfrutar con su familia, Gebru pas√≥ el D√≠a de Acci√≥n de Gracias escribiendo un documento rebatiendo cada objeci√≥n. La respuesta de Google fue glacial: retracta el paper o quita los nombres de los autores de Google.

Gebru respondi√≥ que lo har√≠a bajo dos condiciones: que le dijeran qui√©n hab√≠a dado el feedback y que establecieran un proceso transparente para revisar investigaci√≥n futura. Si no, renunciar√≠a despu√©s de ayudar a su equipo en la transici√≥n.

Google interpret√≥ esto como una renuncia inmediata y la despidi√≥ esa misma noche, mientras ella manejaba en medio de un viaje por carretera en Texas.

## La Normalizaci√≥n de la Censura

Lo verdaderamente revelador del caso Gebru no es solo que Google haya censurado el paper. Es que lo hizo en un contexto donde OpenAI ya hab√≠a legitimado el secretismo corporativo.

Con GPT-2, OpenAI argument√≥ que el modelo era "demasiado peligroso" para publicar. Con GPT-3, publicaron un paper "sanitizado" con casi cero informaci√≥n sobre c√≥mo fue entrenado, algo que antes se consideraba el m√≠nimo acad√©mico.

El mensaje a la industria fue claro: ya no es necesario ser transparente. Y Google aprendi√≥ muy bien esa la lecci√≥n.

Despu√©s de ChatGPT, la opacidad se volvi√≥ norma. En 2023, investigadores de Stanford crearon un tracker de transparencia para evaluar si las empresas de IA revelaban informaci√≥n b√°sica sobre sus modelos: n√∫mero de par√°metros, datos de entrenamiento, verificaci√≥n independiente.

Las diez empresas evaluadas ‚ÄîOpenAI, Google, Anthropic incluidas‚Äî sacaron F. La nota m√°s alta fue 54%.

## Big AI = Big Tobacco

Para Karen Hao, el despido de Gebru es un s√≠mbolo de un problema mayor: Big AI est√° siguiendo el camino de Big Tobacco. Concentraci√≥n de recursos, distorsi√≥n de la investigaci√≥n, censura de estudios cr√≠ticos.

Dos investigadores lo plantearon directamente: la industria de IA est√° actuando exactamente como las tabacaleras en los a√±os 60 y 70, suprimiendo investigaci√≥n que amenaza sus intereses comerciales.

El caso ilustra tres patolog√≠as del ecosistema actual:

**1. Concentraci√≥n total del poder de investigaci√≥n**: Solo las Big Tech tienen los recursos computacionales para entrenar modelos masivos. Esto les da control absoluto sobre qu√© se investiga y qu√© se publica.

**2. Diversidad inexistente en espacios de poder**: Gebru hab√≠a co-fundado "Black in AI"", organizaci√≥n que visibilizaba investigaci√≥n relevante para comunidades marginadas. Su equipo de IA √©tica era uno de los m√°s diversos de Google. Su despido elimin√≥ esa voz cr√≠tica.

**3. Cero protecci√≥n para whistleblowers**: Tres meses despu√©s, Google despidi√≥ tambi√©n a Meg Mitchell, co-l√≠der del equipo de IA √©tica, por descargar emails relacionados con el caso Gebru. El mensaje: quien hable, se va.

## La Obsesi√≥n de Jeff Dean

Hay un detalle interesantes en toda esta historia: [Jeff Dean](https://es.wikipedia.org/wiki/Jeff_Dean), el legendario ingeniero de Google, uno de sus primeros empleados y figura venerada en la comunidad de IA, nunca pudo superar el paper de "Stochastic Parrots".

A√±os despu√©s del despido de Gebru, Dean segu√≠a obsesionado con una secci√≥n espec√≠fica: el c√°lculo de la huella de carbono del modelo Evolved Transformer, basado en la investigaci√≥n de [Emma Strubell](https://es.wikipedia.org/wiki/Jeff_Dean).

Dean argumentaba que Strubell hab√≠a sobreestimado las emisiones de Google 88 veces porque us√≥ GPUs est√°ndar en sus c√°lculos, cuando Google usaba TPUs m√°s eficientes. Empleados de Google se burlaban en privado diciendo que esta objeci√≥n estar√≠a inscrita en su tumba.

El problema: Strubell nunca afirm√≥ estar calculando las emisiones reales de Google. Google nunca hab√≠a publicado esos datos. Strubell hizo una estimaci√≥n basada en hardware est√°ndar de la industria, precisamente porque Google no era transparente.

Entonces Google culp√≥ a Gebru por citar estimaciones p√∫blicas en lugar de datos internos que la empresa nunca hab√≠a revelado y que, adem√°s, le prohibieron buscar antes de despedirla.

Un [catch-22](https://en.wikipedia.org/wiki/Catch-22_(logic)) perfecto cuyo √∫nico resultado posible era la censura.

Dean posteriormente invit√≥ a Strubell a participar en un paper "corrigiendo" sus supuestos errores. Aunque la investigadora acept√≥ con entusiasmo la invitaci√≥n, todo se arruin√≥ por la actitud avasalladora de Google. Dave Patterson, investigador senior de la empresa, le dej√≥ claro a Strubell, en una tensa reuni√≥n, que ser√≠a "mejor para su carrera" participar.

Strubell interpret√≥ las palabras como una amenaza velada y se retir√≥ de la colaboraci√≥n.

![](./empire_of_ai.jpg)

## El Colapso de la Integridad Cient√≠fica

La opacidad creciente tiene una consecuencia devastadora: la erosi√≥n de la integridad cient√≠fica.

El fundamento de la investigaci√≥n en deep learning es simple: los datos de entrenamiento deben ser diferentes de los datos de prueba. Sin capacidad de auditar los datos de entrenamiento, este paradigma colapsa.

¬øC√≥mo sabemos que un modelo realmente "mejora su inteligencia" cuando obtiene mejores puntajes en benchmarks? No lo sabemos. Podr√≠a simplemente estar recitando respuestas memorizadas.

Cuando OpenAI finalmente acept√≥ compartir detalles t√©cnicos de GPT-3, solo para que investigadores de Google pudieran calcular su huella de carbono y compararla con la de Strubell, ya hab√≠a pasado m√°s de un a√±o desde su lanzamiento.

Este nivel de secretismo es incompatible con el m√©todo cient√≠fico.

## ¬øQu√© Futuro Estamos Construyendo?

El paper "Stochastic Parrots" terminaba con una pregunta central: ¬øQu√© tipo de futuro estamos construyendo con IA? ¬øPor y para qui√©n?

El caso Gebru responde esa pregunta de manera brutal: un futuro donde Big Tech controla no solo la tecnolog√≠a, sino tambi√©n qu√© se puede decir sobre ella. Donde la cr√≠tica cient√≠fica se subordina a los intereses comerciales. Donde investigadores talentosos son expulsados por hacer las preguntas inc√≥modas, pero necesarias.

Gebru hab√≠a sido contratada precisamente para liderar investigaci√≥n cr√≠tica en IA √©tica. Google us√≥ su presencia para pulir su imagen p√∫blica de "empresa responsable". Pero cuando esa investigaci√≥n amenaz√≥ la narrativa corporativa sobre los LLM, justo cuando OpenAI los hab√≠a puesto de moda, Google la elimin√≥ sin contemplaciones.

Una carta abierta en solidaridad con Gebru reuni√≥ casi 7.000 firmas, incluyendo 2.700 empleados de Google. El CEO Sundar Pichai emiti√≥ una disculpa tibia. Se cre√≥ un nuevo "centro de expertise en IA responsable".

Pero nada cambi√≥ en lo fundamental.

Despu√©s de ChatGPT, la censura y el secretismo se aceleraron. OpenAI dej√≥ de publicar en conferencias acad√©micas. El resto de la industria sell√≥ el acceso a detalles t√©cnicos de modelos comerciales.

La investigaci√≥n en IA, que naci√≥ en universidades abiertas donde el conocimiento se compart√≠a libremente, ahora est√° cautiva en corporaciones que la tratan como propiedad intelectual clasificada.

No es casualidad. Es estrategia. Mientras t√∫ y yo no podamos auditar estos sistemas, mientras investigadores independientes no puedan verificar sus afirmaciones, mientras la cr√≠tica cient√≠fica sea silenciada mediante despidos y amenazas veladas, las Big Tech pueden vender lo que quieran sobre sus modelos de IA.

Y nosotros solo podemos confiar en su palabra.

---

*Este art√≠culo est√° basado en el cap√≠tulo 7 "Science in Captivity" del libro ["Empire of AI"](https://amzn.to/46JUBWE) de Karen Hao, que documenta en detalle el caso Timnit Gebru y sus implicaciones para el futuro de la investigaci√≥n en IA.*

*Si te interesa recibir an√°lisis como este directamente en tu correo, suscr√≠bete a mi [newsletter](newsletter.lnds.net/).*
